\documentclass{article}

\usepackage{inputenc}[utf8]
\usepackage[T1]{fontenc}
\usepackage[a4paper]{geometry}
\usepackage{fancyhdr}
\usepackage{url}
\usepackage{hyperref}
\usepackage[ngerman]{babel}
\usepackage{graphicx}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amssymb}

% Generate external pdf and import
\pgfplotsset{width=9cm,compat=1.9}

\title{{\Huge Aufgabenblatt 03}\\Einführung in die Kryptographie PS}
\author{Andreas Schlager}


\begin{document}
    \pagestyle{fancy}
    \fancyhead{}
    \fancyhead[L]{Aufgabenblatt 03}
    \fancyhead[R]{Einführung in die Kryptographie}
    \fancyfoot{}
    \fancyfoot[L]{Andreas Schlager}
    \fancyfoot[R]{\thepage}
    \maketitle
    \tableofcontents
    \section{Aufgabe 10}
    \textit{Fortsetzung Aufgabe 9.) Simulieren sie verschiedene Arten von biometrischer Varianz, 
    die sich als gleichverteilte Bitfehler steigender Anzahl oder Bursts (gehäufte
    Fehler an einer oder mehreren Stellen) manifestieren. Wenden sie verschiedene
    Hamming-Codes zur Fehlerkorrektur an. Dokumentieren sie die Auswirkung von
    verschiedenen Fehlerarten (Quantität, Qualität) auf die Möglichkeit, den Schlüssel
    tatsächlich korrekt zu erzeugen.}\vspace*{1em}\newline
    Ein regulärer Hamming-Code ist in der Lage 1-Bit-Fehler zu korrigieren und 2-Bit-Fehler zu erkennen.
    Würde man die Länge des Codes genau an den Schlüssel $S$ oder die biometrischen Daten $X$ anpassen,
    könnte man demnach auch nur einen Bit-Fehler durch die Messung korrigieren. In den meisten Fällen variiert die Messung
    jedoch stärker als nur ein einzelnes verändertes Bit. Die Lösung ist den Schlüssel in mehrere Blöcke
    entsprechender Größen aufzuteilen und anschließend aus jedem einen Hamming Code zu generieren. Treten 
    nun Fehler in unterschiedlichen Blöcken auf, können diese ohne Probleme korrigiert werden, wobei
    zwei oder mehr Fehler im gleichen Block nach wie vor nicht behoben werden können. In wie viele Blöcke man 
    den Schlüssel segmentieren sollte, hängt davon ab welchen Grad der Fehlerkorrektur und wie viel Redundanz
    man möchte. Kleine Blöcke, also kürzere Hamming Codes, führen zu einer größeren Redundanz 
    (siehe Abbildung \ref{fig:redundancyvsdata}.) und somit zu einer verbesserten Messtoleranz, 
    die benötigte Datenmenge steigt jedoch ebenfalls.
    
    \input{figures/hamming_redundancy.tex}

    Um diesen Trade-off zu visualisieren, werden unterschiedliche Hamming-Code-Größen verglichen, wobei verschiedene
    Arten von Bitfehlern (gleichverteilt, oder burst) auftreten. Hierbei ist $H(n, d)$ ein $n$-Bit langer Hamming-Code
    mit $d$ Datenbits und $n-d$ Paritätsbits.
    In der Simulation werden folgende Hamming-Code Blockgrößen getestet:
    \[
        \begin{array}{ccccc}
            H(8,4) & H(16,11) & H(32,26) & H(64,57) & H(128,120)
        \end{array}
    \]
    \subsection{Code}
    Der Funktionsweise des Code aus Aufgabe 9.) ist im wesentlichen gleich geblieben, wobei die Hamming-Code Bibliothek um die 
    Funktionalität der Segmentierung in die gewünschten Blockgrößen erweitert wurde. So führt ein Aufruf
    der Funktion \verb|hamming::encode(data, n)| dazu, dass die Daten in entsprechenden viele $H(n,d)$ Codes
    aufgeteilt werden. Angenommen es werden $x$-Bits in $H(n,d)$ Blöcke überführt, gäbe es pro Block $d$ Datenbits.
    Die Anzahl der dadurch entstehenden Blöcke entspricht
    \begin{equation} \label{eq:1}
        \#\text{Blöcke}=\left\lceil\frac{x}{d}\right\rceil.
    \end{equation}
    \begin{figure}[h]
        % \includegraphics[width=\textwidth]{img/segmentation.pdf}
        \caption{Segmentierung von 128-Bits auf fünf $H(32,26)$ Codes}
        \label{fig:segmentation}
    \end{figure}
    Füllen die Daten die Hamming Blöcke nicht vollständig aus, werden sie mit Nullen ergänzt. Teilt man also einen
    128-Bit Datenblock auf fünf $H(32,26)$ Codes auf, wären im letzten Block noch 24-Datenbits und zwei Füllbits.
    Für das FCS wird wieder ein zufälliger 128-Bit Schlüssel $S$ erstellt, welcher dann in mehrere Hamming-Code-Blöcke kodiert wird.
    Die Blockgrößer wird über \verb|hamming_block_size| bestimmt und nimmt in der Simulation die Werte $\left[8,16,32,64,128\right]$
    an. Die Blöcke werden anschließend in einen durchgängigen Block umgewandelt, um die Kombination mit der biometrischen Probe
    zu vereinfachen.
    \begin{verbatim}
fn fcs_run(hamming_block_size: usize, flip_amount: usize) -> bool {
    let s = rng().random_iter().take(16).collect::<Vec<u8>>();
    let hs = hash(&s);
    let c = hamming::encode(&s, hamming_block_size)
        .expect("encode failed")
        .to_continuous();
    // ...
}
    \end{verbatim}
    Als nächstes wird eine biometrische Probe $X$ zufällig generiert, wobei die Probe gleich groß wie der Schlüssel $S$ ist.
    Um $X$ und $C$ zu kombinieren, müssen beide gleich groß sein. Durch die Paritäts- und Füllbits ist allerdings $|C|>|X|$.
    Um dieses Problem zu umgehen wird $X$ ebenfalls Hamming-kodiert und dadurch ebenfalls durch Füllbits ergänzt.
    \begin{verbatim}
let x = rng().random_iter().take(s.len()).collect::<Vec<u8>>();
let x = hamming::encode(&x, hamming_block_size)
    .expect("encode failed")
    .to_continuous();

let w = fuse(&c, &x);
    \end{verbatim}
    Die Messvariation wird simuliert, indem zufällige Bits von $X$ gekippt werden. Für diesen Zweck gibt es zwei unterschiedliche
    Funktionen: \verb|random_bit_flip| und \verb|burst_error|.
    \begin{verbatim}
let y = random_bit_flip(&x, flip_amount);
// oder
let y = burst_error(&x, rng().random_range(0..s.len()*8), flip_amount);
    \end{verbatim}
    Anschließend wird $Y$ mit $W$ kombiniert um das Codewort $C'$ zu erhalten, welches fehlerkorrigiert wird. Zu diesem Zeitpunkt
    besteht $C'$ noch immer aus den einzelnen Hamming-Blöcken. Um den Schlüssel $S'$ zu erhalten, müssen die Datenbits aus dem
    Code extrahiert werden.
    \begin{verbatim}
let c_prime = fuse(&w, &y);

let mut ecc = HammingCode::from_continuous(c_prime, hamming_block_size);
ecc.error_correct();

let s_prime = ecc.extract(s.len() * 8);
    \end{verbatim}
    Zuletzt werden die Hashwerte verglichen, um festzustellen, ob der Authentifizierung erfolgreich war.
    \begin{verbatim}
let hs_prime = hash(&s_prime);
hs == hs_prime
    \end{verbatim}
    \subsubsection{Simulation}
    Für die Simulation werden 1-15 Bitfehler (in gleichverteilt und burst Variante) mit den unterschiedlichen
    Hamming Codes jeweils 50 Mal getestet. Die Anzahl der erfolgreichen Authentifizierungsversuche wird gezählt und auf
    der Konsole ausgegeben.
    \begin{verbatim}
for flip_amount in 1..=15 {
    print!("{flip_amount}\t");
    for block_size in [8, 16, 32, 64, 128] {
        let successful_attempts = (0..50)
            .map(|_| fcs_run(block_size, flip_amount))
            .filter(|&r| r)
            .count();
        print!("{successful_attempts}\t");
    }
    println!();
}
    \end{verbatim}
    \subsection{Ergebnisse}
    Im Allgemeinen ergab die Simulation, dass die Toleranz der Messung durch die Segmentierung in kleinere Codes
    steigt. Das war zu erwarten, da die Redundanz bei kleineren Codes größer ist und somit mehr Bits für die
    Fehlerkorrektur verfügbar sind. Die Simulation ergab außerdem, dass Burstfehler für Hamming Codes Schwierigkeiten
    bereiten, da häufiger mehrere Fehler in den gleichen Blöcken auftreten.
    \subsubsection{Gleichverteilte Bitfehler}
    Bei den Ergebnissen (siehe Tab. \ref{tab:uniform_result}.) ist klar zu erkennen, dass durch die Segmentierung 
    mehrere zufällig Fehler erkannt und korrigiert werden konnten.
    Nicht besonders überraschend ist auch, dass 1-Bit Fehler von jedem Code in allen 50 Versuchen korrigiert 
    wurden. Der $H(8,4)$-Code war in der Lage im Durchschnitt die meisten Bitfehler zu korrigieren, da die
    Hälfte des Codes Redundanz ist. Außerdem ist erkenntlich: Je länger der Code (also weniger Redundanz), desto
    weniger Bits können korrigiert werden. Eine Überraschung war, dass der $H(128,120)$-Code es manchmal schaffte
    einen großen Bitfehler zu korrigieren. Um zu verstehen wieso es trotz des großen Fehlers zu einer erfolgreichen
    Authentifizierung kam, muss man sich überlegen wie der Schlüssel in den Blöcken positioniert ist.\newline
    Der Schlüssel wurde aus 128 zufälligen Bits konstruiert und in einen $H(128,120)$-Code umgewandelt.
    Wegen der Gleichung \ref{eq:1} ergeben sich zwei 128-Bit Blöcke, mit Platz für jeweils 120-Datenbits.
    Der Schlüssel füllt nun die 120-Bit des ersten Blocks vollständig aus und die übrigen acht Bit kommen
    in den zweiten Block. Das bedeutet, der zweite Block beinhaltet acht Schlüsselbits und \textbf{112 Füllbits}.
    Um die Berechnung zu vereinfachen wird näherungsweise angenommen, dass der zweite Block ausschließlich aus
    Füllbits besteht. Sei $X$ eine Zufallsvariable, die den Block beschreibt, in dem ein zufälliges Bit 
    gekippt ist. Da beide Blöcke gleich groß sind und die Wahrscheinlichkeit der Position des gekippten Bits
    gleichverteilt ist, folgt für die Wahrscheinlichkeiten $P(X=x)$
    \[
        P(X=1) = P(X=2) = 0.5.
    \]
    Sei außerdem $Y$ eine Zufallsvariable, welche die Anzahl der gekippten Bits im ersten Block beschreibt.
    Betrachtet man beispielsweise den Fall von 6-Bitfehlern, dann ist für eine erfolgereiche Authentifizierung
    höchsten ein Fehler im ersten Block erlaubt.
    \begin{align*}
        P(Y \leq 1) &= P(Y = 0) + P(Y = 1) = P(X=2)^{6} + {6\choose 1}P(X=1)^1P(X=2)^{5}\\
        &=0.5^{6} + 6\cdot 0.5^1 \cdot 0.5^5 = 0.109375
    \end{align*}
    \[
        \mathbb{E}\left[\text{"'erfolgreiche Authentifizierungen"'}\right] =  50 \cdot 0.109375 = 5.46875
    \]
    Im Schnitt erwartet man also bei dieser Implementierung des FCS und sechs gleichverteilten Bitfehlern 
    ungefähr $5.47$ erfolgereiche Authentifizierungen. Diese Tatsache spiegelt sich auch in der Ergebnistabelle 
    \ref{tab:uniform_result} wieder.

    \input{figures/result_uniform_error.tex}
    \subsubsection{Burstfehler}  
    Die Simulation-Ergebnisse (siehe Tabelle \ref{tab:burst_result}.) zeigen, dass Burstfehler eine besondere Herausforderung für die Hamming-Codes darstellen. 
    Da sich die Fehler über mehrere aufeinanderfolgende Bits erstrecken, kommt es häufiger vor, dass mehrere Fehler 
    innerhalb desselben Blocks auftreten. Besonders bei längeren Codes mit geringerer Redundanz führte dies dazu, 
    dass die Fehlerkorrektur versagte. Bei kürzeren Codes war die Wahrscheinlichkeit höher, dass die Fehler auf mehrere 
    Blöcke verteilt wurden, wodurch die Korrekturmechanismen etwas greifen konnten. Dennoch zeigte sich, 
    dass Hamming-Codes grundsätzlich weniger robust gegenüber Burstfehlern sind als gegenüber zufällig verteilten 
    Bitfehlern.
    \input{figures/result_burst.tex}
    \newpage
    \section{Aufgabe 11}
    \textit{Implementieren sie den Caesar Cipher (Slide 14) mit z als Variable/Schlüssel für
    Buchstaben-orientierte Textverschlüsselung. Führen sie eine (Ciphertext-only) brute
    Force Attacke gegen einen verschlüsselten Text aus (unter der Annahme der Wert
    von z wäre nicht bekannt) und überlegen sie sich ein oder mehrere Kriterien um den
    tatsächlich richtigen Plaintext unter allen erzeugten zu eruieren (und wenden sie das
    alles auf Beispiele an).}\vspace*{1em}\newline
    Der Caesar Cipher ist ein Verschlüsselungsverfahren, bei dem jeder Buchstabe im Plaintext um einen feste Anzahl
    im Alphabet verschoben wird. Die Anzahl, um die verschoben wird, ist der Schlüssel $z$. Da es im englischen Alphabet
    26 verschiedene Buchstaben geht, gibt es nur \textbf{25 mögliche Schlüssel} (0 wird ignoriert, weil eine Verschiebung 
    keinen Effekt hätte). Aufgrund der geringen Anzahl an möglichen Schlüssel ist der Caesar Cipher leicht durch Brute-Force 
    Verfahren zu knacken.
    \subsection{Code}
    Für die Implementierung werden nur Buchstaben im englischen Alphabet betrachtet, die sich auch in der ASCII-Tabelle befinden.
    Um den Ciphertext zu berechnen, wird über jedes Zeichen des Plaintexts iteriert. Ist das Zeichen ein Buchstabe, wird
    er im Alphabet um den Schlüssel verschoben und an den Ausgabetext angehängt. Falls das Zeichen kein Buchstabe ist, wird 
    nicht verschoben und es kommt unverändert an die Ausgabe.
    \begin{verbatim}
fn caesar_cipher(text: &str, key: usize) -> String {
    let key = (key % 26) as u8;
    text.chars()
        .map(|c| shift_letter(c, key))
        .fold(String::new(), |mut out, c| {
            out.push(c);
            out
        })
    \end{verbatim}
    Jeder Character hat einen zugewiesenen Wert entsprechend der ASCII-Tabelle. Die Kleinbuchstaben befinden sich z.B. an
    den Stellen 97-122. Um einen Buchstaben zu verschieben, ist es hilfreich mit einem Wertebereich von 0-25 zu rechnen.,
    weshalb der Character zuerst um den Wert von 'a' oder 'A' reduziert wird. Anschließend kann der Schlüsselwert
    modulo 26 addiert werden. Zuletzt wird der Character noch auf den ursprünglichen Wertebereich zurück verschoben. 
    \begin{verbatim}
fn shift_letter(c: char, shift: u8) -> char {
    match c {
        'A'..='Z' => ((c as u8 - b'A' + shift) % 26 + b'A') as char,
        'a'..='z' => ((c as u8 - b'a' + shift) % 26 + b'a') as char,
        _ => c,
    }
}
    \end{verbatim}
    \subsubsection{Ciphertext-only Angriffe}
    Bei einem Ciphertext-only Angriff steht dem Angreifer nur der verschlüsselte Text zur Verfügung.
    Das Ziel ist es den Schlüssel mittels verschiedener Ansätze zu finden.
    Im Fall des Caesar Ciphers gibt es nur 25 mögliche Schlüssel, weshalb der Keyspace
    leicht vollständig durchsucht werden kann. Im weiteren werden zwei verschiedene
    Brute-Force Ansätze betrachtet: Manuelle Überprüfung und automatisierte Schlüsselsuche mittels 
    Buchstaben Häufigkeiten.

    \paragraph{Manuelle Suche}
    Der denkbar einfachste Ansatz einer Brute-Force Attacke ist alle Schlüssel auszuprobieren und
    manuell (also durch anschauen) jede mögliche Entschlüsselung zu bewerten.
    Da in diesem Fall nur 25 Ergebnisse entstehen, kann der korrekte Plaintext leicht entdeckt werden.
    Für dieses Beispiel wird der folgenden Plaintext um einen zufällig gewählten Schlüssel verschoben:
    \begin{quote}
        "` In other words, if two keys are equal, their hashes must be equal. Violating this property is a logic error. "'
    \end{quote}
    \begin{verbatim}
let cipher = caesar_cipher(plain, rng().random_range(0..26));
    \end{verbatim}
    Danach wird der Ciphertext mit allen möglichen Schlüsseln entschlüsselt und jedes Ergebnis auf die Konsole ausgegeben.
    Ist ein Buchstabe um einen Schlüssel $k$ verschoben worden, dann kann er mit dem gleichen Algorithmus
    und $26 - k$ entschlüsselt werden.
    \begin{verbatim}
(0..26)
    .map(|shift| (caesar_cipher(&cipher, 26 - shift), shift))
    .for_each(|(s, key)| println!("Key {:02}: {}", key, s));
    \end{verbatim}
    \begin{figure}
        \includegraphics[width=\textwidth]{text_results/console_out1.png}
        \caption{Ausgabe des Brute-Force Angriffs (Manual Checking)}
        \label{fig:console_manual_checking}
    \end{figure}
    Wie in Abbildung \ref{fig:console_manual_checking} durch manuelles Überprüfen zu sehen ist,
    war der Schlüssel $k=20$. In der zweiten Zeile ist außerdem zu sehen, dass
    eine Verschiebung mit 0 keine Auswirkungen hat. Der größte Vorteil dieses
    Verfahrens ist natürlich die Einfachheit. Ist der Keyspace allerdings etwas größer,
    z.B. 2000 mögliche Schlüssel, wird die manuelle Überprüfung schon mühsam. Man wäre
    allerdings noch weit von der Größer der Keyspaces praktischer Algorithmen entfernt.
    Dieser Ansatz verliert also schnell seine Effektivität.
    \paragraph{Buchstaben Häufigkeiten} Ein effektiverer Ansatz könnte jedem Entschlüsselungsergebnis
    eine Art Bewertung geben. Die Bewertung sollte dann ein Maß dafür sein, für wie wahrscheinlich das Ergebnis
    der korrekte Plaintext ist. Dadurch entsteht ein automatisiertes Ranking der Schlüsselkandidaten.
    Die besten Ergebnisse können wieder manuell überprüft werden. Eine mögliche Metrik für
    die Qualität des Ergebnisses, ist die Häufigkeit der auftretenden Buchstaben. In jeder Sprache
    kommen gewisse Buchstaben häufiger vor als andere. Man kann ein Referenz Histogram der
    englischen Sprache mit dem enstehenden Histogram des Entschlüsselungsergebnisses vergleichen
    und die Abweichung berechnen. Je höher die Abweichung ist, desto geringer soll der 
    Score sein. Als Referenz habe ich ein Buchstaben Histrogram der englischen Sprache verwendet.
    \begin{verbatim}
let english_letter_frequencies: [f32; 26] = [
    0.08167, 0.01492, 0.02782, 0.04253, 0.12702, 0.02228, 0.02015, 
    0.06094, 0.06966, 0.00153, 0.00772, 0.04025, 0.02406, 0.06749, 
    0.07507, 0.01929, 0.00095, 0.05987, 0.06327, 0.09056, 0.02758, 
    0.00978, 0.02360, 0.00150, 0.01974, 0.00074,
];
    \end{verbatim}
    Jede Stelle im Array entspricht der relativen Häufigkeit eines Buchstabens, wobei
    'e' der häufigste mit $\approx 12.7\%$ ist.
    Für den Angriff werden zuerst alle Entschlüsselungsergebnisse gesammelt und deren Histogramme berechnet.
    Dann wird jedes mit der Referenz verglichen um den Score zu erhalten. Zuletzt
    wird noch nach dem Score in absteigender Reihenfolge sortiert.\newpage
    \begin{verbatim}
let decrypt_attempts = (0..26)
    .map(|shift| caesar_cipher(&cipher, 26 - shift))
    .collect::<Vec<_>>();

let histograms = decrypt_attempts
    .iter()
    .map(|plain| letter_frequency(plain))
    .collect::<Vec<_>>();

let mut scores = histograms
    .iter()
    .enumerate()
    .map(|(key, h)| (key, score(&english_letter_frequencies, h)))
    .collect::<Vec<_>>();
scores.sort_by(|(_, a), (_, b)| b.partial_cmp(&a).unwrap());
    \end{verbatim}
    Für die Berechnung des Scores werden die absoluten Abweichung jedes Buchstabens zu seinem
    Referenzwert berechnet, die Summe gebildet und durch eine Exponentialfunktion
    in den Wertebereich $\left[0,1\right]$ transformiert. Ein Score von 1 bedeutet,
    dass es keine Abweichungen zwischen den Histogrammen gab, ein niedriger Score bedeutet
    hohe Abweichungen.
    \begin{verbatim}
fn score(reference: &[f32; 26], h: &[f32; 26]) -> f32 {
    E.powf(
        -reference
            .iter()
            .zip(h)
            .map(|(&x, &y)| (x - y).abs())
            .sum::<f32>(),
    )
}
    \end{verbatim}
    Wie in Abbildung \ref{fig:buchstaben_haufigkeit} zu sehen ist, wird der Schlüssel $k=20$ am besten bewertet.
    Für lange Texte funktioniert diese Methode sehr gut, da sich die Häufigkeit der Referenz immer weiter annähern
    Wie allerdings in Abbildung \ref{fig:buchstaben_bad} zu sehen ist, führen sehr kurze Texte wie "` World "' zu 
    Fehleinschätzungen. Das liegt daran, dass wenige Buchstaben nicht genug Information bieten. Das korrekte
    Ergebnis wird nur an 5. Stelle angezeigt.
    \begin{figure}
        \includegraphics[width=\textwidth]{text_results/console_out2.png}
        \caption{Ausgabe des Brute-Force Angriffs (Buchstaben Häufigkeit)}
        \label{fig:buchstaben_haufigkeit}
    \end{figure}
    \begin{figure}
        \includegraphics[scale=0.5]{text_results/console_out3.png}
        \caption{Ausgabe des Brute-Force Angriffs (Buchstaben Häufigkeit) bei kurzen Wörtern}
        \label{fig:buchstaben_bad}
    \end{figure}
\end{document}